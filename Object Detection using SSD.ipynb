{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577f3530",
   "metadata": {},
   "source": [
    "# Object Detection using SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bad67",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d1b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from data import BaseTransform, VOC_CLASSES as labelmap\n",
    "from ssd import build_ssd\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595da92",
   "metadata": {},
   "source": [
    "### Defining a function that will do the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37920200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(frame, net, transform): \n",
    "    ''' # We define a detect function that will take as inputs, a frame, a ssd neural network,\n",
    "    and a transformation to be applied on the images, and that will return the frame with the detector rectangle.'''\n",
    "    \n",
    "    # We get the height and the width of the frame.\n",
    "    height, width = frame.shape[:2] \n",
    "    \n",
    "    \n",
    "    #Applying transformations..\n",
    "    # We apply the transformation to our frame.(returns two output but we need only one i.e. transformed frame)\n",
    "    frame_t = transform(frame)[0] \n",
    "    \n",
    "    # We convert the frame into a torch tensor. (color code :- grb as ssd is trained on this color code)\n",
    "    x = torch.from_numpy(frame_t).permute(2, 0, 1) \n",
    "    \n",
    "    # We add a fake dimension corresponding to the batch. and converting to tensor variable\n",
    "    x = Variable(x.unsqueeze(0)) \n",
    "    #transformations over...\n",
    "    \n",
    "    # We feed the neural network ssd with the image and we get the output y.\n",
    "    y = net(x) \n",
    "    \n",
    "    # We create the detections tensor contained in the output y.\n",
    "    detections = y.data\n",
    "    \n",
    "    #detections=[batch,number of classes,number of occurence,[score,x0,y0,x1,y1]]\n",
    "    \n",
    "    # We create a tensor object of dimensions [width, height, width, height].\n",
    "    #First (width,height) --> top left corner\n",
    "    #Second (width,height) --> bottom right corner\n",
    "    scale = torch.Tensor([width, height, width, height])\n",
    "    \n",
    "    # For every class:\n",
    "    for i in range(detections.size(1)): \n",
    "        \n",
    "        # We initialize the loop variable j that will correspond to the occurrences of the class.\n",
    "        j = 0 \n",
    "        \n",
    "        # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.\n",
    "        #[batch,class,occurence,score]\n",
    "        while detections[0, i, j, 0] >= 0.6: \n",
    "            \n",
    "            # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.\n",
    "            #Need to convert tensor to numpy to plot rectangle in openCV\n",
    "            pt = (detections[0, i, j, 1:] * scale).numpy() \n",
    "            \n",
    "            # We draw a rectangle around the detected object.\n",
    "            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2) \n",
    "            \n",
    "             # We put the label of the class right above the rectangle.\n",
    "            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # We increment j to get to the next occurrence.\n",
    "            j += 1 \n",
    "            \n",
    "    # We return the original frame with the detector rectangle and the label around the detected object.        \n",
    "    return frame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55d367",
   "metadata": {},
   "source": [
    "### Creating the SSD neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c447a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pulkit\\Downloads\\Object-Detection-using-SSD\\ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # We create an object that is our neural network ssd.\n",
    "net = build_ssd('test')\n",
    "\n",
    "# We get the weights of the neural network from another one that is pretrained\n",
    "# map_location = lambda storage, loc: storage need to put them as parameters while loading model\n",
    "net.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ff53f",
   "metadata": {},
   "source": [
    "### Creating the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c247ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an object of the BaseTransform class, a class that will do the required transformations\n",
    "#so that the image can be the input of the neural network\n",
    "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0)) \n",
    "\n",
    "#net.size -> transformation of frame according to ssd\n",
    "#other numbers are the numbers that ssd is trained on..no need to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38941449",
   "metadata": {},
   "source": [
    "### Doing some Object Detection on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f6349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pulkit\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# We open the video.\n",
    "reader = imageio.get_reader('funny_dog.mp4') \n",
    "\n",
    "# We get the fps frequence (frames per second).\n",
    "fps = reader.get_meta_data()['fps'] \n",
    "\n",
    "# We create an output video with this same fps frequence.\n",
    "writer = imageio.get_writer('output.mp4', fps = fps)\n",
    "\n",
    "# We iterate on the frames of the output video:\n",
    "for i, frame in enumerate(reader): \n",
    "    \n",
    "    # We call our detect function (defined above) to detect the object on the frame.\n",
    "    frame = detect(frame, net.eval(), transform) \n",
    "    \n",
    "    # We add the next frame in the output video.\n",
    "    writer.append_data(frame) \n",
    "    \n",
    "     # We print the number of the processed frame.\n",
    "    print(i)\n",
    "    \n",
    "# We close the process that handles the creation of the output video.\n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d4bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
